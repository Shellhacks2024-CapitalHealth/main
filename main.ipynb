{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process multiple CSV files in a folder\n",
    "def process_financial_data(folder_path):\n",
    "    people_data = {}  # Dictionary to store data for each person (file)\n",
    "    \n",
    "    # Iterate over all CSV files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):  \n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Load the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Convert the 'Date' column to datetime format\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            \n",
    "            # Split the data into income and spending based on 'Transaction_Type'\n",
    "            income_data = df[df['Transaction_Type'] == 'Deposit'][['Date', 'Amount']].copy()\n",
    "            spending_data = df[df['Transaction_Type'] == 'Withdrawal'][['Date', 'Amount']].copy()\n",
    "            \n",
    "            # Store the processed data in the dictionary using the filename as the key\n",
    "            people_data[filename] = {\n",
    "                'income': income_data,\n",
    "                'spending': spending_data\n",
    "            }\n",
    "    \n",
    "    return people_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GAN**\n",
    "1) Generator (Nueral Net that creates what it thinks is replicated data)\n",
    "2) Discriminator (Nueral Net that is fed both the real and fake data and chooses which one is the most realistis)\n",
    "3) Real/Fake (Sends Back Propogration to the Nueral Nets to edit them depending on the outcome of the Discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Noise for Generator\n",
    "def latent_dim():\n",
    "    #Generate function to optimize randomness\n",
    "    return 10\n",
    "\n",
    "#alpha function for LeakyReLU paramater\n",
    "def alpha():\n",
    "    #Generate function to optimize dying nuerons\n",
    "    return 0.2\n",
    "\n",
    "#input shape function for discriminator\n",
    "def inputShape(dataFrame):\n",
    "    return len(dataFrame.columns)-1\n",
    "\n",
    "#drop paramater for discriminator\n",
    "def dropout():\n",
    "    return 0.3\n",
    "\n",
    "#Define batch size\n",
    "def batch():\n",
    "    return 32\n",
    "\n",
    "#Data values\n",
    "def meanIncome(dataFrame):\n",
    "    return dataFrame['Deposits'].mean()\n",
    "\n",
    "def stddevIncome(dataFrame):\n",
    "    return dataFrame['Deposits'].std()\n",
    "\n",
    "def meanExpense(dataFrame):\n",
    "    return dataFrame['Withdrawl'].mean()\n",
    "\n",
    "def stddevExpense(dataFrame):\n",
    "    return dataFrame['Withdrawl'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator\n",
    "def generator(noise):\n",
    "    model = models.Sequential() #Sequential NN\n",
    "    model.add(layers.Dense(128, activation='relu', input_dim=noise))  #Layer of 128 Neurons\n",
    "    model.add(layers.BatchNormalization()) #Normaliztion of previous output layer\n",
    "    model.add(layers.LeakyReLU(alpha=alpha()))  #Correct Dead Neurons\n",
    "    model.add(layers.Dense(256, activation='relu', input_dim=noise))  #Layer of 256 Neurons\n",
    "    model.add(layers.BatchNormalization()) #Normaliztion of previous output layer\n",
    "    model.add(layers.LeakyReLU(alpha=alpha()))  #Correct dead Neurons\n",
    "    model.add(layers.Dense(2, activation = 'tanh')) #Output for income and expense\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discriminator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminator\n",
    "def discriminator(inputShape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(256, activation = 'relu', input_shape=inputShape))\n",
    "    model.add(layers.LeakyReLU(alpha=alpha()))\n",
    "    model.add(layers.Dropout(dropout()))    #Dropout layer meaining dropout% of neurons will be ignored\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.LeakyReLU(alpha=alpha()))\n",
    "    model.add(layers.Dropout(dropout()))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))    #sigmoid activation function gives output of 0->1, 1 means real\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining GAN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator()\n",
    "discriminator = discriminator()\n",
    "\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#GAN Model\n",
    "discriminator.trainable = False #Freeze \n",
    "gan_input = layers.Input(shape=(latent_dim()))\n",
    "generated_data = generator(gan_input)\n",
    "gan_output = discriminator(generated_data)\n",
    "gan = models.Model(gan_input, gan_output)\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Real Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRealData(batch):\n",
    "    income = np.random.normal(loc = meanIncome(), scale = stddevIncome(), size = batch)\n",
    "    expense = np.random.normal(loc = meanExpense(), scale = stddevExpense(), size = batch)\n",
    "    return np.stack((income, expense), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, gan, latent_dim, epochs, batch_size):\n",
    "    #Training loop\n",
    "    for epoch in range(epochs):\n",
    "        #Generate 'Real Data'\n",
    "        real_data = generateRealData(batch_size)\n",
    "        real_labels = np.ones((batch_size, 1))\n",
    "\n",
    "        #Generate Fake Data\n",
    "        noise = np.random.normal(0,1, (batch_size, latent_dim))\n",
    "        fake_data = generator.predict(noise)\n",
    "        fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "        # Train discriminator\n",
    "        discriminator_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "        discriminator_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
    "        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
    "\n",
    "        # Train generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        misleading_labels = np.ones((batch_size, 1))  # Labels for generator training\n",
    "        generator_loss = gan.train_on_batch(noise, misleading_labels)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss}, Generator Loss: {generator_loss}\")\n",
    "\n",
    "#Run training\n",
    "train_gan(generator, discriminator, gan, latent_dim, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
